{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Anaconda2\\\\Lib\\\\site-packages\\\\blpfunctions')\n",
    "from blpfunctions import blpfunctions as blp\n",
    "#import blpfunctions as blp\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pandas.tseries.offsets import *\n",
    "from holidays_jp import CountryHolidays\n",
    "import dateutil.rrule as RR\n",
    "import sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _load_data(data, n_prev = 62):  \n",
    "    \"\"\"\n",
    "    data should be pd.DataFrame()\n",
    "    \"\"\"\n",
    "    docX, docY = [], []\n",
    "    for i in range(len(data)-n_prev):\n",
    "        docX.append(data.iloc[i:i+n_prev-1].as_matrix())\n",
    "        docY.append(data.iloc[i+n_prev].as_matrix())\n",
    "        #docX.append(data.iloc[i:i+n_prev])\n",
    "        #docY.append(data.iloc[i+n_prev])\n",
    "    alsX = np.array(docX)\n",
    "    alsY = np.array(docY)\n",
    "\n",
    "    return alsX, alsY\n",
    "\n",
    "def train_test_split(df, test_size=0.082, n_prev = 62):  \n",
    "    \"\"\"\n",
    "    This just splits data to training and testing parts\n",
    "    \"\"\" \n",
    "    ntrn = round(len(df) * (1 - test_size),0)\n",
    "    ntrn = int(ntrn)\n",
    "    X_train, y_train = _load_data(df.iloc[0:ntrn], n_prev)\n",
    "    X_test, y_test = _load_data(df.iloc[ntrn:], n_prev)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def proc_data(stock_list, window):      \n",
    "    fld_l = [\"VOLUME\"]\n",
    "    event_l = [\"TRADE\"]\n",
    "    iv = 5\n",
    "    ed1 = \"2017-09-25T15:00:00\"\n",
    "    days = 60\n",
    "    test_size = 0.115\n",
    "    Train_list = []\n",
    "    test_list =[]\n",
    "    y_dict = {}\n",
    "    y_test_dict = {}\n",
    "    for ind,stock in enumerate(stock_list):\n",
    "        avc, vc = horiz_stock_volcurve([stock +' JT Equity'],event_l,ed1,days,iv,fld_l)\n",
    "        X_train, y_train, X_test, y_test = train_test_split(vc[[stock]], test_size, window)\n",
    "        aX_train, ay_train, aX_test, ay_test = train_test_split(avc[[stock]], test_size, window)\n",
    "        Train_list.append(X_train)\n",
    "        Train_list.append(aX_train)\n",
    "        y_dict[stock] = y_train\n",
    "        #y_list.append(ay_train)\n",
    "        test_list.append(X_test)  \n",
    "        test_list.append(aX_test) \n",
    "        y_test_dict[stock]=y_test\n",
    "        #y_test_list.append(ay_test)\n",
    "    return Train_list, y_dict, test_list, y_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def horiz_stock_volcurve(ind, event, edate, numdays, interval,fld_lst):\n",
    "    #sec_list = blp.get_index(ind)\n",
    "    #sec_list = ['5801 JT Equity']\n",
    "    sec_list = ind\n",
    "    volcurves = pd.DataFrame()\n",
    "    fmt = \"%Y-%m-%d\" + 'T' + \"%H:%M:%S\"  #Assumes no milliseconds\n",
    "    endDateTime = dt.datetime.strptime(edate, fmt)\n",
    "    #skip SQ and holidays (Actually turns out cannot skip SQ days in blp call)\n",
    "    day1 = dt.datetime(int(edate[0:4]),1,1)\n",
    "    sq = list(RR.rrule(RR.MONTHLY,byweekday=RR.FR,bysetpos=2,dtstart=day1,until=endDateTime))\n",
    "    hols = list(zip(*CountryHolidays.get('JP', int(edate[0:4])))[0])\n",
    "    skipdays = hols #+ sq if want to skip SQ days\n",
    "    bday_jp = CustomBusinessDay(holidays=skipdays)\n",
    "    startDateTime = endDateTime.replace(hour=9) - numdays*bday_jp\n",
    "    numdays = pd.date_range(startDateTime, endDateTime, freq=bday_jp).nunique()\n",
    "    sdate = startDateTime.strftime(fmt)\n",
    "    vc = pd.DataFrame()\n",
    "    avc = pd.DataFrame()\n",
    "    for stock in sec_list:\n",
    "        volcurves=blp.get_Bars(stock, event, sdate, edate, interval, fld_lst)\n",
    "    volcurves.rename(columns={'VOLUME':stock},inplace=True)\n",
    "    #process the raw data into historical averages\n",
    "    volcurves.rename(columns=lambda x: x[:4], inplace=True)\n",
    "    timevect = pd.Series(volcurves.index.values)\n",
    "    timeframet = timevect.to_frame()\n",
    "    timeframet.columns = ['date']\n",
    "    timeframet.set_index(timevect,inplace=True)\n",
    "    timeframet['bucket'] = timeframet['date'].apply(lambda x: dt.datetime.strftime(x, '%H:%M:%S'))\n",
    "    timeframet['date'] = timeframet['date'].apply(lambda x: dt.datetime.strftime(x, '%Y-%m-%d'))  \n",
    "    absvolcurve=timeframet.join(volcurves).fillna(0).reset_index(level=0,drop=True)\n",
    "    absvolcurve = absvolcurve[absvolcurve.bucket != '15:10:00']\n",
    "    absvolcurve['bucket2'] = absvolcurve['date'] + absvolcurve['bucket']\n",
    "    buckets = sorted(absvolcurve['bucket'].unique())\n",
    "    dates = sorted(absvolcurve['date'].unique())\n",
    "    df = pd.DataFrame([dat+buk for dat in dates for buk in buckets],columns=['bucket2'])\n",
    "    absvolcurve = df.merge(absvolcurve, how= 'left', on='bucket2').fillna(0).reset_index(level=0,drop=True) \n",
    "    volcurve = absvolcurve.copy()\n",
    "    sums = absvolcurve.groupby('date')[stock[:4]].sum()\n",
    "    absvolcurve['sums'] = absvolcurve['date'].map(sums)\n",
    "    volcurve['ratio'] = absvolcurve[stock[:4]]/absvolcurve['sums']\n",
    "    volcurve['sumcum'] = volcurve.groupby('date')['ratio'].cumsum()\n",
    "    absvolcurve = absvolcurve.drop(['sums','bucket2','date','bucket'],axis=1)\n",
    "    volcurve = volcurve.drop(['ratio',stock[:4],'bucket2','date','bucket'],axis=1)\n",
    "    volcurve = volcurve.rename(columns = {'sumcum':stock[:4]})     \n",
    "    volcurve = volcurve.interpolate()\n",
    "    avc[stock[:4]] = absvolcurve[stock[:4]]\n",
    "    vc[stock[:4]] = volcurve[stock[:4]]\n",
    "            \n",
    "    return avc.fillna(method='bfill'), vc.fillna(method='bfill')\n",
    "    #return absvolcurve.fillna(method='bfill'), volcurve.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fld_lst = [\"VOLUME\"]\n",
    "ind = [\"NKY\"]\n",
    "event = [\"TRADE\"]\n",
    "interval = 5\n",
    "edate = \"2017-09-25T15:00:00\"\n",
    "numdays = 60\n",
    "avc, vc = horiz_stock_volcurve(ind, event, edate, numdays, interval,fld_lst)\n",
    "window = 60\n",
    "length_of_sequences=60\n",
    "feats = 1\n",
    "X_train, y_train, X_test, y_test = train_test_split(vc[['5801']], n_prev = length_of_sequences)\n",
    "aX_train, ay_train, aX_test, ay_test = train_test_split(avc[['5801']], n_prev = length_of_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_BDLSTM_model([feats,window,1])\n",
    "model.fit([np.array(X_train), np.array(aX_train)], np.array(y_train), batch_size=5, epochs=50, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p = model.predict([X_test,aX_test],batch_size=50)\n",
    "train_mse, test_mse = BDLSTM_model_score(model, X_train, y_train, aX_train, aX_test, X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print train_mse, test_mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Activation, Input, LSTM, Dropout, Bidirectional, Lambda, merge\n",
    "from keras.layers.core import Flatten, Permute, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from attention_lstm import AttentionLSTMWrapper\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n",
    "SINGLE_ATTENTION_VECTOR = False\n",
    "\n",
    "def attention_3d_block(inputs,shape):\n",
    "    #inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    #input_dim = int(inputs.shape[2])\n",
    "    input_dim = shape[1]\n",
    "    TIME_STEPS=shape[0]\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, TIME_STEPS))(a)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    if SINGLE_ATTENTION_VECTOR:\n",
    "        a = Lambda(lambda x: K.mean(x, axis=1), name='dim_reduction')(a)\n",
    "        a = RepeatVector(input_dim)(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = merge([inputs, a_probs], name='attention_mul', mode='mul')\n",
    "    return output_attention_mul\n",
    "\n",
    "def build_BDLSTM_model(layers):\n",
    "    d = 0.0\n",
    "    num_lstm = 128\n",
    "    \n",
    "    shape = [layers[1], layers[0]]\n",
    "      \n",
    "    shared_lstm = LSTM(num_lstm, dropout=d, recurrent_dropout=d, return_sequences=True, consume_less='mem')\n",
    "    \n",
    "    s1 = Input(shape=shape)\n",
    "    s2 = Input(shape=shape)\n",
    "    \n",
    "    attention_mul1 = attention_3d_block(s1,shape)\n",
    " \n",
    "    \n",
    "    x1 = shared_lstm(attention_mul1)\n",
    "    #x1 = BatchNormalization()(x1)\n",
    "    x2 = LSTM(num_lstm, dropout=d, recurrent_dropout=d, return_sequences=True, consume_less='mem')(s2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "       \n",
    "    merged = concatenate([x1,x2])\n",
    "    \n",
    "    merged = Dense(32, kernel_initializer=\"uniform\", activation='relu')(merged)\n",
    "    merged = Dropout(0.6)(merged)\n",
    "    \n",
    "    merged = Dense(32, kernel_initializer=\"uniform\", activation='relu')(merged)\n",
    "    merged = Dropout(0.6)(merged)\n",
    "    \n",
    "    preds = Dense(layers[0], kernel_initializer=\"uniform\", activation='linear')(merged)\n",
    "        \n",
    "    start = time.time()\n",
    "    model = Model(inputs = [s1,s2], outputs = [preds])\n",
    "    model.compile(loss='mse',optimizer='nadam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def BDLSTM_model_score(model, X_train, y_train, aX_train, aX_test, X_test, y_test):\n",
    "    trainScore = model.evaluate([X_train,aX_train], y_train ,batch_size=50, verbose=0)\n",
    "    #print('Train Score: %.5f MSE (%.4f RMSE)' % (trainScore[0], np.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate([X_test, aX_test], y_test, batch_size=50, verbose=0)\n",
    "    #print('Test Score: %.5f MSE (%.4f RMSE)' % (testScore[0], np.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window = 3\n",
    "feats = len(relcurve.columns)\n",
    "X_train, y_train, X_test, y_test = load_data(relcurve, window)\n",
    "aX_train, ay_train, aX_test, ay_test = load_data(abscurve, window)\n",
    "\n",
    "model = build_model([feats,window,1])\n",
    "model.fit([X_train, aX_train], y_train, batch_size=5, epochs=200, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(X_test[-1])\n",
    "#diff=[]\n",
    "#ratio=[]\n",
    "p = model.predict([X_test, aX_test],batch_size=4096)\n",
    "print (p.shape)\n",
    "# for each data index in test data\n",
    "# for u in range(len(y_test)):\n",
    "#     # pr = prediction day u\n",
    "#     pr = p[u][0]\n",
    "#     # (y_test day u / pr) - 1\n",
    "#     ratio.append((y_test[u]/pr)-1)\n",
    "#     diff.append(abs(y_test[u]- pr))\n",
    "#     # print(u, y_test[u], pr, (y_test[u]/pr)-1, abs(y_test[u]- pr))\n",
    "#     # Last day prediction\n",
    "#     # print(p[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt2\n",
    "#p = model.predict([X_test, aX_test],batch_size=4096)\n",
    "plt2.plot(p,color='red', label='Prediction')\n",
    "plt2.plot(y_test,color='blue', label='Actual')\n",
    "plt2.legend(loc='best')\n",
    "plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build, Train, and Save results per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Cluster Cross-Stock Bidirectional LSTM\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Activation, Input, LSTM, Dropout, Bidirectional, Lambda\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "def build_cluster_cross_stock_model(layers):\n",
    "    d = 0.3\n",
    "    num_lstm = 256\n",
    "    shape = [layers[1], layers[0]]\n",
    "    \n",
    "    lstm = LSTM(num_lstm, recurrent_dropout=d)\n",
    "    #lstm = LSTM(num_lstm, dropout=d)\n",
    "    batchnorm = BatchNormalization()\n",
    "    \n",
    "    input_list = []\n",
    "    for num in range(0,2*layers[2]):\n",
    "        input_list.append(Input(shape=shape))\n",
    "    print('Using {} tensors as input.'.format(len(input_list)))    \n",
    "        \n",
    "    lstm_layers=[]\n",
    "    for i in range(0,layers[2]):\n",
    "        lstm_layers.append(lstm)\n",
    "    print('Using {} interlaced LSTM layers on those inputs.'.format(len(lstm_layers)))\n",
    "    \n",
    "    lstm_outputs=[]\n",
    "    for i in range(1,2*layers[2],2):\n",
    "        id_lstm_layer = i//2\n",
    "        shared_lstm = lstm_layers[id_lstm_layer]\n",
    "        lstm_outputs.append(shared_lstm(input_list[i - 1]))\n",
    "        lstm_outputs.append(batchnorm(shared_lstm(input_list[i])))\n",
    "        print('LSTM layer id = {} is linked to inputs whose ids are {} and {}.'.format(id_lstm_layer, i - 1, i))\n",
    "                            \n",
    "    merged = Lambda(lambda x: K.stack(x,axis=1),output_shape=(2*layers[2], 2*num_lstm))(lstm_outputs)\n",
    "    merged = Flatten()(merged)                        \n",
    "    merged = Dense(128, kernel_initializer=\"uniform\", activation='relu')(merged)\n",
    "    merged = Dropout(0.7)(merged)\n",
    "    merged = Dense(128, kernel_initializer=\"uniform\", activation='relu')(merged)\n",
    "    merged = Dropout(0.7)(merged)\n",
    "    preds = Dense(layers[0], kernel_initializer=\"uniform\", activation='linear')(merged)\n",
    "    \n",
    "    start = time.time()\n",
    "    model = Model(inputs = input_list, outputs = [preds])\n",
    "    model.compile(loss='mse',optimizer='nadam', metrics=['accuracy'])\n",
    "    print(\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "def cross_stock_model_score(model, Train, y, test, test_y):\n",
    "    trainScore = model.evaluate(Train, y ,batch_size=50, verbose=0)\n",
    "    #print('Train Score: %.5f MSE (%.4f RMSE)' % (trainScore[0], np.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(test, test_y, batch_size=50, verbose=0)\n",
    "    #print('Test Score: %.5f MSE (%.4f RMSE)' % (testScore[0], np.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def go(clust,mode):\n",
    "    \n",
    "    window = 62\n",
    "    feats = 1\n",
    "    \n",
    "    print('Training in %s mode' % mode)\n",
    "    stock_list = asses.item()[clust]\n",
    "    output = pd.DataFrame(columns=['stock','bst_val_score','train_rmse','test_rmse'],index=stock_list)\n",
    "    output['stock'] = stock_list\n",
    "    num_stocks = len(stock_list)\n",
    "    #tb = TensorBoard(log_dir='./tblogs', histogram_freq=0, write_graph=True, write_grads=False, write_images=False)\n",
    "    \n",
    "    Train_list, y_dict, test_list, y_test_dict = proc_data(stock_list, window)\n",
    "    \n",
    "    if mode == 'cross':\n",
    "        model = build_cluster_cross_stock_model([feats,window-1,num_stocks])\n",
    "        STAMP = 'Horiz_Cross-stock-cluster-' + str(clust)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "        \n",
    "    elif mode == 'single':\n",
    "        model = build_BDLSTM_model([feats,window-1,num_stocks])\n",
    "        model.summary()\n",
    "        STAMP = 'Horiz_BDLSTM_61_10_50-cluster-' + str(clust)\n",
    "        bst_model_path = STAMP + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "     \n",
    "    for ind,stock in enumerate(stock_list): \n",
    "        if mode == 'cross':\n",
    "            model.load_weights(bst_model_path)\n",
    "            hist = model.fit(Train_list, y_dict[stock], batch_size=61, epochs=100, validation_split=0.0943, verbose=True,\n",
    "                  callbacks=[early_stopping])\n",
    "            bst_val_score=min(hist.history['val_loss'])\n",
    "            #p = model.predict(test_list,batch_size=50)\n",
    "            train_mse, test_mse = cross_stock_model_score(model, Train_list, y_dict[stock], test_list, y_test_dict[stock])\n",
    "            \n",
    "        elif mode == 'single':\n",
    "            X_train = Train_list[2*ind]\n",
    "            aX_train = Train_list[2*ind+1]           \n",
    "            X_test = test_list[2*ind]\n",
    "            aX_test = test_list[2*ind+1]\n",
    "            y_train = y_dict[stock]\n",
    "            y_test = y_test_dict[stock]\n",
    "            #model.load_weights(bst_model_path)\n",
    "            hist = model.fit([X_train, aX_train], y_train, batch_size=61, epochs=100, validation_split=0.0943, verbose=True,\n",
    "                  callbacks=[early_stopping])\n",
    "            bst_val_score=min(hist.history['val_loss'])\n",
    "            #p = model.predict([X_test,aX_test],batch_size=50)\n",
    "            train_mse, test_mse = BDLSTM_model_score(model, X_train, y_train, aX_train, aX_test, X_test, y_test)     \n",
    "      \n",
    "        output['bst_val_score'][stock] = bst_val_score\n",
    "        output['train_rmse'][stock] = np.sqrt(train_mse)\n",
    "        output['test_rmse'][stock] = np.sqrt(test_mse)\n",
    "    \n",
    "        print('Stock %s: Best Val Score: %.4f, Train Score: %.4f RMSE, Test Score %.4f RMSE' \n",
    "              % (stock[:4], bst_val_score, np.sqrt(train_mse), np.sqrt(test_mse)))\n",
    "        \n",
    "    model.save_weights(bst_model_path)\n",
    "    output.to_csv(STAMP + '-Horizontal.csv.gz', index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(clusters):\n",
    "    #test on smallest cluster\n",
    "    clusters =[4]\n",
    "    mode = 'single'\n",
    "    #mode = 'cross'\n",
    "    \n",
    "    for clust in clusters:\n",
    "        go(clust,mode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in single mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, implementation=1, return_sequences=True, dropout=0.0, recurrent_dropout=0.0)`\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:25: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:44: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, implementation=1, return_sequences=True, dropout=0.0, recurrent_dropout=0.0)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compilation Time : ', 0.00800013542175293)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_49 (InputLayer)            (None, 61, 1)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "permute_12 (Permute)             (None, 1, 61)         0           input_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)             (None, 1, 61)         0           permute_12[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_23 (Dense)                 (None, 1, 61)         3782        reshape_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "attention_vec (Permute)          (None, 61, 1)         0           dense_23[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "input_50 (InputLayer)            (None, 61, 1)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "attention_mul (Merge)            (None, 61, 1)         0           input_49[0][0]                   \n",
      "                                                                   attention_vec[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                   (None, 61, 128)       66560       input_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                   (None, 61, 128)       66560       attention_mul[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNor (None, 61, 128)       512         lstm_27[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)     (None, 61, 256)       0           lstm_26[0][0]                    \n",
      "                                                                   batch_normalization_25[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "dense_24 (Dense)                 (None, 61, 32)        8224        concatenate_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 61, 32)        0           dense_24[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 61, 32)        1056        dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 61, 32)        0           dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 61, 1)         33          dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 146,727\n",
      "Trainable params: 146,471\n",
      "Non-trainable params: 256\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_26 to have 3 dimensions, but got array with shape (3231L, 1L)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-88863693969a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0masses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clusters.npy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-99cd0699d23b>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(clusters)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mclust\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclusters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mgo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclust\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-18cbeba1e767>\u001b[0m in \u001b[0;36mgo\u001b[1;34m(clust, mode)\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[1;31m#model.load_weights(bst_model_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             hist = model.fit([X_train, aX_train], y_train, batch_size=61, epochs=100, validation_split=0.0943, verbose=True,\n\u001b[1;32m---> 46\u001b[1;33m                   callbacks=[early_stopping])\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mbst_val_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;31m#p = model.predict([X_test,aX_test],batch_size=50)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1427\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1430\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1310\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1311\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32mC:\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    125\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    128\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_26 to have 3 dimensions, but got array with shape (3231L, 1L)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    asses = np.load('clusters.npy')\n",
    "    clusters = asses.item().keys()\n",
    "    run(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fld_l = [\"VOLUME\"]\n",
    "event_l = [\"TRADE\"]\n",
    "iv = 5\n",
    "ed1 = \"2017-09-25T15:00:00\"\n",
    "days = 60\n",
    "#abscurve, relcurve = horiz_stock_volcurve(['5801 JT Equity'],event_l,ed1,days,iv,fld_l)\n",
    "X_train, y_train, X_test, y_test = train_test_split(relcurve, 0.115, 62)\n",
    "aX_train, ay_train, aX_test, ay_test = train_test_split(abscurve, 0.115, 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(abscurve['bucket'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_BDLSTM_model([3,61,1])\n",
    "model.fit([X_train, aX_train], y_train, batch_size=5, epochs=1, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano \n",
    "theano.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9a7881f870d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
